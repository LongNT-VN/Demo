{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Khoi tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .appName('movieRecommendationPySpark') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Load du lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (\n",
    "    spark.read.csv(\n",
    "        path = \"../data/ml-25m/ratings.csv\",\n",
    "        sep=\",\", header=True,quote='\"',schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "    ).select(\"userId\", \"movieId\", \"rating\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol =\"userId\",\n",
    "    itemCol =\"movieId\",\n",
    "    ratingCol = \"rating\",\n",
    ")\n",
    "(training_data, validation_data) = ratings.randomSplit([8.0,2.0])\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "model = als.fit(training_data)\n",
    "predictions = model.transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelRecNormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|31563 |104841 |3.5   |3.4680753 |\n",
      "|31563 |96966  |4.0   |4.109942  |\n",
      "|31563 |106452 |3.0   |3.9050436 |\n",
      "|31563 |109487 |5.0   |3.8087015 |\n",
      "|31563 |111759 |3.5   |3.5323467 |\n",
      "|31563 |109374 |5.0   |4.07057   |\n",
      "|31563 |103539 |3.0   |3.424638  |\n",
      "|31563 |106766 |5.0   |3.7999783 |\n",
      "|31563 |101525 |3.5   |3.7429621 |\n",
      "|31563 |111364 |3.5   |2.6357195 |\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8046279138022526\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions.na.drop())\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userFactors=model.userFactors\n",
    "# itemFactors = model.itemFactors\n",
    "# userFactors.sort('id').show(5,False)\n",
    "# itemFactors.sort('id').show(5,False)\n",
    "# import numpy as np\n",
    "# user91Features = model.userFactors.filter(f.col('id')==91).select(f.col('features')).rdd.flatMap(lambda x:x).collect()[0]\n",
    "# item471Features = model.itemFactors.filter(f.col('id')==471).select(f.col('features')).rdd.flatMap(lambda x:x).collect()[0]\n",
    "\n",
    "# print(user91Features)\n",
    "# print(item471Features)\n",
    "# print('Predicted rating of user 91 for movie 471: ' + str(np.dot(user91Features, item471Features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .appName('movieRecommendationPySpark') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir('checkpoint')\n",
    "\n",
    "ratings = (\n",
    "    spark.read.csv(\n",
    "        path = \"../data/ml-25m/ratings.csv\",\n",
    "        sep=\",\", header=True,quote='\"',schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "    ).select(\"userId\", \"movieId\", \"rating\")\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "als = ALS(\n",
    "    userCol =\"userId\",\n",
    "    itemCol =\"movieId\",\n",
    "    ratingCol = \"rating\",\n",
    ")\n",
    "(training_data, validation_data) = ratings.randomSplit([8.0,2.0])\n",
    "\n",
    "param_grid = (ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10]) \\\n",
    "    .addGrid(als.maxIter, [20]) \\\n",
    "    .addGrid(als.regParam, [0.05,0.1]) \\\n",
    "    .build()\n",
    ")\n",
    "# Tell Spark how to evaluate model performance\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "# Build cross validation step using CrossValidator\n",
    "cv = CrossValidator(estimator=als,estimatorParamMaps=param_grid,evaluator=evaluator,numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = cv.fit(training_data)\n",
    "best_model = model_cv.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"modelRecBest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044351138766392\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(validation_data)\n",
    "rmse = evaluator.evaluate(predictions.na.drop())\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db772567c4567f4b3d1f56ed53e5d7229d15382f6d3dfec57e0c55f7b5cf2dd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
